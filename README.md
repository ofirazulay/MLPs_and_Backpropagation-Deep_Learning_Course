# MLPs and Backpropagation

### Deep_Learning_Course



<br/>

#### Description: 
##### implementation of a multi-layer perceptron using 2 Variations:
###### 1) Purely NumPy routines.
The network consists of a series of linear layers with ReLU activation functions followed by a final linear layer and
softmax activation. As a loss function, we use the common cross-entropy loss for classification
tasks. To optimize the network we used the mini-batch stochastic gradient descent
algorithm. 

The Implemention is in the files:

• train_mlp_numpy.py

• modules.py

• mlp_numpy.py

###### 2) pytorch
We implemented the same MLP in pytorch.

The Implemention is in the files:

• train_mlp_pytorch.py

• mlp_pytorch.py

<br/>
   
###
##### Keywords: 
deep learning, MLP, Backpropagation, project, network, multi-layer perceptron, pytorch, NumPy
###

##### By Ofir Azulay & Moshe Cohen
[LinkedIn](https://www.linkedin.com/in/ofir-azulay/)
##
